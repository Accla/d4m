// File generated by hadoop record compiler. Do not edit.
package org.apache.hadoop.record;

public class RecRecordOld extends org.apache.hadoop.record.Record {
  private static final org.apache.hadoop.record.meta.RecordTypeInfo _rio_recTypeInfo;
  private static org.apache.hadoop.record.meta.RecordTypeInfo _rio_rtiFilter;
  private static int[] _rio_rtiFilterFields;
  static {
    _rio_recTypeInfo = new org.apache.hadoop.record.meta.RecordTypeInfo("RecRecordOld");
    _rio_recTypeInfo.addField("name", org.apache.hadoop.record.meta.TypeID.StringTypeID);
    _rio_recTypeInfo.addField("ivec", new org.apache.hadoop.record.meta.VectorTypeID(org.apache.hadoop.record.meta.TypeID.LongTypeID));
    _rio_recTypeInfo.addField("svec", new org.apache.hadoop.record.meta.VectorTypeID(new org.apache.hadoop.record.meta.VectorTypeID(new org.apache.hadoop.record.meta.StructTypeID(org.apache.hadoop.record.RecRecord0.getTypeInfo()))));
    _rio_recTypeInfo.addField("inner", new org.apache.hadoop.record.meta.StructTypeID(org.apache.hadoop.record.RecRecord0.getTypeInfo()));
    _rio_recTypeInfo.addField("strvec", new org.apache.hadoop.record.meta.VectorTypeID(new org.apache.hadoop.record.meta.VectorTypeID(new org.apache.hadoop.record.meta.VectorTypeID(org.apache.hadoop.record.meta.TypeID.StringTypeID))));
    _rio_recTypeInfo.addField("i1", org.apache.hadoop.record.meta.TypeID.FloatTypeID);
    _rio_recTypeInfo.addField("map1", new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.ByteTypeID, org.apache.hadoop.record.meta.TypeID.StringTypeID));
    _rio_recTypeInfo.addField("mvec1", new org.apache.hadoop.record.meta.VectorTypeID(new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.IntTypeID, org.apache.hadoop.record.meta.TypeID.LongTypeID)));
    _rio_recTypeInfo.addField("mvec2", new org.apache.hadoop.record.meta.VectorTypeID(new org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID.IntTypeID, org.apache.hadoop.record.meta.TypeID.LongTypeID)));
  }
  
  private String name;
  private java.util.ArrayList<Long> ivec;
  private java.util.ArrayList<java.util.ArrayList<org.apache.hadoop.record.RecRecord0>> svec;
  private org.apache.hadoop.record.RecRecord0 inner;
  private java.util.ArrayList<java.util.ArrayList<java.util.ArrayList<String>>> strvec;
  private float i1;
  private java.util.TreeMap<Byte,String> map1;
  private java.util.ArrayList<java.util.TreeMap<Integer,Long>> mvec1;
  private java.util.ArrayList<java.util.TreeMap<Integer,Long>> mvec2;
  public RecRecordOld() { }
  public RecRecordOld(
    final String name,
    final java.util.ArrayList<Long> ivec,
    final java.util.ArrayList<java.util.ArrayList<org.apache.hadoop.record.RecRecord0>> svec,
    final org.apache.hadoop.record.RecRecord0 inner,
    final java.util.ArrayList<java.util.ArrayList<java.util.ArrayList<String>>> strvec,
    final float i1,
    final java.util.TreeMap<Byte,String> map1,
    final java.util.ArrayList<java.util.TreeMap<Integer,Long>> mvec1,
    final java.util.ArrayList<java.util.TreeMap<Integer,Long>> mvec2) {
    this.name = name;
    this.ivec = ivec;
    this.svec = svec;
    this.inner = inner;
    this.strvec = strvec;
    this.i1 = i1;
    this.map1 = map1;
    this.mvec1 = mvec1;
    this.mvec2 = mvec2;
  }
  public static org.apache.hadoop.record.meta.RecordTypeInfo getTypeInfo() {
    return _rio_recTypeInfo;
  }
  public static void setTypeFilter(org.apache.hadoop.record.meta.RecordTypeInfo rti) {
    if (null == rti) return;
    _rio_rtiFilter = rti;
    _rio_rtiFilterFields = null;
    org.apache.hadoop.record.RecRecord0.setTypeFilter(rti.getNestedStructTypeInfo("RecRecord0"));
  }
  private static void setupRtiFields()
  {
    if (null == _rio_rtiFilter) return;
    // we may already have done this
    if (null != _rio_rtiFilterFields) return;
    int _rio_i, _rio_j;
    _rio_rtiFilterFields = new int [_rio_rtiFilter.getFieldTypeInfos().size()];
    for (_rio_i=0; _rio_i<_rio_rtiFilterFields.length; _rio_i++) {
      _rio_rtiFilterFields[_rio_i] = 0;
    }
    java.util.Iterator<org.apache.hadoop.record.meta.FieldTypeInfo> _rio_itFilter = _rio_rtiFilter.getFieldTypeInfos().iterator();
    _rio_i=0;
    while (_rio_itFilter.hasNext()) {
      org.apache.hadoop.record.meta.FieldTypeInfo _rio_tInfoFilter = _rio_itFilter.next();
      java.util.Iterator<org.apache.hadoop.record.meta.FieldTypeInfo> _rio_it = _rio_recTypeInfo.getFieldTypeInfos().iterator();
      _rio_j=1;
      while (_rio_it.hasNext()) {
        org.apache.hadoop.record.meta.FieldTypeInfo _rio_tInfo = _rio_it.next();
        if (_rio_tInfo.equals(_rio_tInfoFilter)) {
          _rio_rtiFilterFields[_rio_i] = _rio_j;
          break;
        }
        _rio_j++;
      }
      _rio_i++;
    }
  }
  public String getName() {
    return name;
  }
  public void setName(final String name) {
    this.name=name;
  }
  public java.util.ArrayList<Long> getIvec() {
    return ivec;
  }
  public void setIvec(final java.util.ArrayList<Long> ivec) {
    this.ivec=ivec;
  }
  public java.util.ArrayList<java.util.ArrayList<org.apache.hadoop.record.RecRecord0>> getSvec() {
    return svec;
  }
  public void setSvec(final java.util.ArrayList<java.util.ArrayList<org.apache.hadoop.record.RecRecord0>> svec) {
    this.svec=svec;
  }
  public org.apache.hadoop.record.RecRecord0 getInner() {
    return inner;
  }
  public void setInner(final org.apache.hadoop.record.RecRecord0 inner) {
    this.inner=inner;
  }
  public java.util.ArrayList<java.util.ArrayList<java.util.ArrayList<String>>> getStrvec() {
    return strvec;
  }
  public void setStrvec(final java.util.ArrayList<java.util.ArrayList<java.util.ArrayList<String>>> strvec) {
    this.strvec=strvec;
  }
  public float getI1() {
    return i1;
  }
  public void setI1(final float i1) {
    this.i1=i1;
  }
  public java.util.TreeMap<Byte,String> getMap1() {
    return map1;
  }
  public void setMap1(final java.util.TreeMap<Byte,String> map1) {
    this.map1=map1;
  }
  public java.util.ArrayList<java.util.TreeMap<Integer,Long>> getMvec1() {
    return mvec1;
  }
  public void setMvec1(final java.util.ArrayList<java.util.TreeMap<Integer,Long>> mvec1) {
    this.mvec1=mvec1;
  }
  public java.util.ArrayList<java.util.TreeMap<Integer,Long>> getMvec2() {
    return mvec2;
  }
  public void setMvec2(final java.util.ArrayList<java.util.TreeMap<Integer,Long>> mvec2) {
    this.mvec2=mvec2;
  }
  public void serialize(final org.apache.hadoop.record.RecordOutput _rio_a, final String _rio_tag)
  throws java.io.IOException {
    _rio_a.startRecord(this,_rio_tag);
    _rio_a.writeString(name,"name");
    {
      _rio_a.startVector(ivec,"ivec");
      int _rio_len1 = ivec.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len1; _rio_vidx1++) {
        long _rio_e1 = ivec.get(_rio_vidx1);
        _rio_a.writeLong(_rio_e1,"_rio_e1");
      }
      _rio_a.endVector(ivec,"ivec");
    }
    {
      _rio_a.startVector(svec,"svec");
      int _rio_len1 = svec.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len1; _rio_vidx1++) {
        java.util.ArrayList<org.apache.hadoop.record.RecRecord0> _rio_e1 = svec.get(_rio_vidx1);
        {
          _rio_a.startVector(_rio_e1,"_rio_e1");
          int _rio_len2 = _rio_e1.size();
          for(int _rio_vidx2 = 0; _rio_vidx2<_rio_len2; _rio_vidx2++) {
            org.apache.hadoop.record.RecRecord0 _rio_e2 = _rio_e1.get(_rio_vidx2);
            _rio_e2.serialize(_rio_a,"_rio_e2");
          }
          _rio_a.endVector(_rio_e1,"_rio_e1");
        }
      }
      _rio_a.endVector(svec,"svec");
    }
    inner.serialize(_rio_a,"inner");
    {
      _rio_a.startVector(strvec,"strvec");
      int _rio_len1 = strvec.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len1; _rio_vidx1++) {
        java.util.ArrayList<java.util.ArrayList<String>> _rio_e1 = strvec.get(_rio_vidx1);
        {
          _rio_a.startVector(_rio_e1,"_rio_e1");
          int _rio_len2 = _rio_e1.size();
          for(int _rio_vidx2 = 0; _rio_vidx2<_rio_len2; _rio_vidx2++) {
            java.util.ArrayList<String> _rio_e2 = _rio_e1.get(_rio_vidx2);
            {
              _rio_a.startVector(_rio_e2,"_rio_e2");
              int _rio_len3 = _rio_e2.size();
              for(int _rio_vidx3 = 0; _rio_vidx3<_rio_len3; _rio_vidx3++) {
                String _rio_e3 = _rio_e2.get(_rio_vidx3);
                _rio_a.writeString(_rio_e3,"_rio_e3");
              }
              _rio_a.endVector(_rio_e2,"_rio_e2");
            }
          }
          _rio_a.endVector(_rio_e1,"_rio_e1");
        }
      }
      _rio_a.endVector(strvec,"strvec");
    }
    _rio_a.writeFloat(i1,"i1");
    {
      _rio_a.startMap(map1,"map1");
      java.util.Set<java.util.Map.Entry<Byte,String>> _rio_es1 = map1.entrySet();
      for(java.util.Iterator<java.util.Map.Entry<Byte,String>> _rio_midx1 = _rio_es1.iterator(); _rio_midx1.hasNext();) {
        java.util.Map.Entry<Byte,String> _rio_me1 = _rio_midx1.next();
        byte _rio_k1 = _rio_me1.getKey();
        String _rio_v1 = _rio_me1.getValue();
        _rio_a.writeByte(_rio_k1,"_rio_k1");
        _rio_a.writeString(_rio_v1,"_rio_v1");
      }
      _rio_a.endMap(map1,"map1");
    }
    {
      _rio_a.startVector(mvec1,"mvec1");
      int _rio_len1 = mvec1.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len1; _rio_vidx1++) {
        java.util.TreeMap<Integer,Long> _rio_e1 = mvec1.get(_rio_vidx1);
        {
          _rio_a.startMap(_rio_e1,"_rio_e1");
          java.util.Set<java.util.Map.Entry<Integer,Long>> _rio_es1 = _rio_e1.entrySet();
          for(java.util.Iterator<java.util.Map.Entry<Integer,Long>> _rio_midx1 = _rio_es1.iterator(); _rio_midx1.hasNext();) {
            java.util.Map.Entry<Integer,Long> _rio_me1 = _rio_midx1.next();
            int _rio_k1 = _rio_me1.getKey();
            long _rio_v1 = _rio_me1.getValue();
            _rio_a.writeInt(_rio_k1,"_rio_k1");
            _rio_a.writeLong(_rio_v1,"_rio_v1");
          }
          _rio_a.endMap(_rio_e1,"_rio_e1");
        }
      }
      _rio_a.endVector(mvec1,"mvec1");
    }
    {
      _rio_a.startVector(mvec2,"mvec2");
      int _rio_len1 = mvec2.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len1; _rio_vidx1++) {
        java.util.TreeMap<Integer,Long> _rio_e1 = mvec2.get(_rio_vidx1);
        {
          _rio_a.startMap(_rio_e1,"_rio_e1");
          java.util.Set<java.util.Map.Entry<Integer,Long>> _rio_es1 = _rio_e1.entrySet();
          for(java.util.Iterator<java.util.Map.Entry<Integer,Long>> _rio_midx1 = _rio_es1.iterator(); _rio_midx1.hasNext();) {
            java.util.Map.Entry<Integer,Long> _rio_me1 = _rio_midx1.next();
            int _rio_k1 = _rio_me1.getKey();
            long _rio_v1 = _rio_me1.getValue();
            _rio_a.writeInt(_rio_k1,"_rio_k1");
            _rio_a.writeLong(_rio_v1,"_rio_v1");
          }
          _rio_a.endMap(_rio_e1,"_rio_e1");
        }
      }
      _rio_a.endVector(mvec2,"mvec2");
    }
    _rio_a.endRecord(this,_rio_tag);
  }
  private void deserializeWithoutFilter(final org.apache.hadoop.record.RecordInput _rio_a, final String _rio_tag)
  throws java.io.IOException {
    _rio_a.startRecord(_rio_tag);
    name=_rio_a.readString("name");
    {
      org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("ivec");
      ivec=new java.util.ArrayList<Long>();
      for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
        long _rio_e1;
        _rio_e1=_rio_a.readLong("_rio_e1");
        ivec.add(_rio_e1);
      }
      _rio_a.endVector("ivec");
    }
    {
      org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("svec");
      svec=new java.util.ArrayList<java.util.ArrayList<org.apache.hadoop.record.RecRecord0>>();
      for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
        java.util.ArrayList<org.apache.hadoop.record.RecRecord0> _rio_e1;
        {
          org.apache.hadoop.record.Index _rio_vidx2 = _rio_a.startVector("_rio_e1");
          _rio_e1=new java.util.ArrayList<org.apache.hadoop.record.RecRecord0>();
          for (; !_rio_vidx2.done(); _rio_vidx2.incr()) {
            org.apache.hadoop.record.RecRecord0 _rio_e2;
            _rio_e2= new org.apache.hadoop.record.RecRecord0();
            _rio_e2.deserialize(_rio_a,"_rio_e2");
            _rio_e1.add(_rio_e2);
          }
          _rio_a.endVector("_rio_e1");
        }
        svec.add(_rio_e1);
      }
      _rio_a.endVector("svec");
    }
    inner= new org.apache.hadoop.record.RecRecord0();
    inner.deserialize(_rio_a,"inner");
    {
      org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("strvec");
      strvec=new java.util.ArrayList<java.util.ArrayList<java.util.ArrayList<String>>>();
      for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
        java.util.ArrayList<java.util.ArrayList<String>> _rio_e1;
        {
          org.apache.hadoop.record.Index _rio_vidx2 = _rio_a.startVector("_rio_e1");
          _rio_e1=new java.util.ArrayList<java.util.ArrayList<String>>();
          for (; !_rio_vidx2.done(); _rio_vidx2.incr()) {
            java.util.ArrayList<String> _rio_e2;
            {
              org.apache.hadoop.record.Index _rio_vidx3 = _rio_a.startVector("_rio_e2");
              _rio_e2=new java.util.ArrayList<String>();
              for (; !_rio_vidx3.done(); _rio_vidx3.incr()) {
                String _rio_e3;
                _rio_e3=_rio_a.readString("_rio_e3");
                _rio_e2.add(_rio_e3);
              }
              _rio_a.endVector("_rio_e2");
            }
            _rio_e1.add(_rio_e2);
          }
          _rio_a.endVector("_rio_e1");
        }
        strvec.add(_rio_e1);
      }
      _rio_a.endVector("strvec");
    }
    i1=_rio_a.readFloat("i1");
    {
      org.apache.hadoop.record.Index _rio_midx1 = _rio_a.startMap("map1");
      map1=new java.util.TreeMap<Byte,String>();
      for (; !_rio_midx1.done(); _rio_midx1.incr()) {
        byte _rio_k1;
        _rio_k1=_rio_a.readByte("_rio_k1");
        String _rio_v1;
        _rio_v1=_rio_a.readString("_rio_v1");
        map1.put(_rio_k1,_rio_v1);
      }
      _rio_a.endMap("map1");
    }
    {
      org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("mvec1");
      mvec1=new java.util.ArrayList<java.util.TreeMap<Integer,Long>>();
      for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
        java.util.TreeMap<Integer,Long> _rio_e1;
        {
          org.apache.hadoop.record.Index _rio_midx1 = _rio_a.startMap("_rio_e1");
          _rio_e1=new java.util.TreeMap<Integer,Long>();
          for (; !_rio_midx1.done(); _rio_midx1.incr()) {
            int _rio_k1;
            _rio_k1=_rio_a.readInt("_rio_k1");
            long _rio_v1;
            _rio_v1=_rio_a.readLong("_rio_v1");
            _rio_e1.put(_rio_k1,_rio_v1);
          }
          _rio_a.endMap("_rio_e1");
        }
        mvec1.add(_rio_e1);
      }
      _rio_a.endVector("mvec1");
    }
    {
      org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("mvec2");
      mvec2=new java.util.ArrayList<java.util.TreeMap<Integer,Long>>();
      for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
        java.util.TreeMap<Integer,Long> _rio_e1;
        {
          org.apache.hadoop.record.Index _rio_midx1 = _rio_a.startMap("_rio_e1");
          _rio_e1=new java.util.TreeMap<Integer,Long>();
          for (; !_rio_midx1.done(); _rio_midx1.incr()) {
            int _rio_k1;
            _rio_k1=_rio_a.readInt("_rio_k1");
            long _rio_v1;
            _rio_v1=_rio_a.readLong("_rio_v1");
            _rio_e1.put(_rio_k1,_rio_v1);
          }
          _rio_a.endMap("_rio_e1");
        }
        mvec2.add(_rio_e1);
      }
      _rio_a.endVector("mvec2");
    }
    _rio_a.endRecord(_rio_tag);
  }
  public void deserialize(final org.apache.hadoop.record.RecordInput _rio_a, final String _rio_tag)
  throws java.io.IOException {
    if (null == _rio_rtiFilter) {
      deserializeWithoutFilter(_rio_a, _rio_tag);
      return;
    }
    // if we're here, we need to read based on version info
    _rio_a.startRecord(_rio_tag);
    setupRtiFields();
    for (int _rio_i=0; _rio_i<_rio_rtiFilter.getFieldTypeInfos().size(); _rio_i++) {
      if (1 == _rio_rtiFilterFields[_rio_i]) {
        name=_rio_a.readString("name");
      }
      else if (2 == _rio_rtiFilterFields[_rio_i]) {
        {
          org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("ivec");
          ivec=new java.util.ArrayList<Long>();
          for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
            long _rio_e1;
            _rio_e1=_rio_a.readLong("_rio_e1");
            ivec.add(_rio_e1);
          }
          _rio_a.endVector("ivec");
        }
      }
      else if (3 == _rio_rtiFilterFields[_rio_i]) {
        {
          org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("svec");
          svec=new java.util.ArrayList<java.util.ArrayList<org.apache.hadoop.record.RecRecord0>>();
          for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
            java.util.ArrayList<org.apache.hadoop.record.RecRecord0> _rio_e1;
            {
              org.apache.hadoop.record.Index _rio_vidx2 = _rio_a.startVector("_rio_e1");
              _rio_e1=new java.util.ArrayList<org.apache.hadoop.record.RecRecord0>();
              for (; !_rio_vidx2.done(); _rio_vidx2.incr()) {
                org.apache.hadoop.record.RecRecord0 _rio_e2;
                _rio_e2= new org.apache.hadoop.record.RecRecord0();
                _rio_e2.deserialize(_rio_a,"_rio_e2");
                _rio_e1.add(_rio_e2);
              }
              _rio_a.endVector("_rio_e1");
            }
            svec.add(_rio_e1);
          }
          _rio_a.endVector("svec");
        }
      }
      else if (4 == _rio_rtiFilterFields[_rio_i]) {
        inner= new org.apache.hadoop.record.RecRecord0();
        inner.deserialize(_rio_a,"inner");
      }
      else if (5 == _rio_rtiFilterFields[_rio_i]) {
        {
          org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("strvec");
          strvec=new java.util.ArrayList<java.util.ArrayList<java.util.ArrayList<String>>>();
          for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
            java.util.ArrayList<java.util.ArrayList<String>> _rio_e1;
            {
              org.apache.hadoop.record.Index _rio_vidx2 = _rio_a.startVector("_rio_e1");
              _rio_e1=new java.util.ArrayList<java.util.ArrayList<String>>();
              for (; !_rio_vidx2.done(); _rio_vidx2.incr()) {
                java.util.ArrayList<String> _rio_e2;
                {
                  org.apache.hadoop.record.Index _rio_vidx3 = _rio_a.startVector("_rio_e2");
                  _rio_e2=new java.util.ArrayList<String>();
                  for (; !_rio_vidx3.done(); _rio_vidx3.incr()) {
                    String _rio_e3;
                    _rio_e3=_rio_a.readString("_rio_e3");
                    _rio_e2.add(_rio_e3);
                  }
                  _rio_a.endVector("_rio_e2");
                }
                _rio_e1.add(_rio_e2);
              }
              _rio_a.endVector("_rio_e1");
            }
            strvec.add(_rio_e1);
          }
          _rio_a.endVector("strvec");
        }
      }
      else if (6 == _rio_rtiFilterFields[_rio_i]) {
        i1=_rio_a.readFloat("i1");
      }
      else if (7 == _rio_rtiFilterFields[_rio_i]) {
        {
          org.apache.hadoop.record.Index _rio_midx1 = _rio_a.startMap("map1");
          map1=new java.util.TreeMap<Byte,String>();
          for (; !_rio_midx1.done(); _rio_midx1.incr()) {
            byte _rio_k1;
            _rio_k1=_rio_a.readByte("_rio_k1");
            String _rio_v1;
            _rio_v1=_rio_a.readString("_rio_v1");
            map1.put(_rio_k1,_rio_v1);
          }
          _rio_a.endMap("map1");
        }
      }
      else if (8 == _rio_rtiFilterFields[_rio_i]) {
        {
          org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("mvec1");
          mvec1=new java.util.ArrayList<java.util.TreeMap<Integer,Long>>();
          for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
            java.util.TreeMap<Integer,Long> _rio_e1;
            {
              org.apache.hadoop.record.Index _rio_midx1 = _rio_a.startMap("_rio_e1");
              _rio_e1=new java.util.TreeMap<Integer,Long>();
              for (; !_rio_midx1.done(); _rio_midx1.incr()) {
                int _rio_k1;
                _rio_k1=_rio_a.readInt("_rio_k1");
                long _rio_v1;
                _rio_v1=_rio_a.readLong("_rio_v1");
                _rio_e1.put(_rio_k1,_rio_v1);
              }
              _rio_a.endMap("_rio_e1");
            }
            mvec1.add(_rio_e1);
          }
          _rio_a.endVector("mvec1");
        }
      }
      else if (9 == _rio_rtiFilterFields[_rio_i]) {
        {
          org.apache.hadoop.record.Index _rio_vidx1 = _rio_a.startVector("mvec2");
          mvec2=new java.util.ArrayList<java.util.TreeMap<Integer,Long>>();
          for (; !_rio_vidx1.done(); _rio_vidx1.incr()) {
            java.util.TreeMap<Integer,Long> _rio_e1;
            {
              org.apache.hadoop.record.Index _rio_midx1 = _rio_a.startMap("_rio_e1");
              _rio_e1=new java.util.TreeMap<Integer,Long>();
              for (; !_rio_midx1.done(); _rio_midx1.incr()) {
                int _rio_k1;
                _rio_k1=_rio_a.readInt("_rio_k1");
                long _rio_v1;
                _rio_v1=_rio_a.readLong("_rio_v1");
                _rio_e1.put(_rio_k1,_rio_v1);
              }
              _rio_a.endMap("_rio_e1");
            }
            mvec2.add(_rio_e1);
          }
          _rio_a.endVector("mvec2");
        }
      }
      else {
        java.util.ArrayList<org.apache.hadoop.record.meta.FieldTypeInfo> typeInfos = (java.util.ArrayList<org.apache.hadoop.record.meta.FieldTypeInfo>)(_rio_rtiFilter.getFieldTypeInfos());
        org.apache.hadoop.record.meta.Utils.skip(_rio_a, typeInfos.get(_rio_i).getFieldID(), typeInfos.get(_rio_i).getTypeID());
      }
    }
    _rio_a.endRecord(_rio_tag);
  }
  public int compareTo (final Object _rio_peer_) throws ClassCastException {
    if (!(_rio_peer_ instanceof RecRecordOld)) {
      throw new ClassCastException("Comparing different types of records.");
    }
    RecRecordOld _rio_peer = (RecRecordOld) _rio_peer_;
    int _rio_ret = 0;
    _rio_ret = name.compareTo(_rio_peer.name);
    if (_rio_ret != 0) return _rio_ret;
    {
      int _rio_len11 = ivec.size();
      int _rio_len21 = _rio_peer.ivec.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len11 && _rio_vidx1<_rio_len21; _rio_vidx1++) {
        long _rio_e11 = ivec.get(_rio_vidx1);
        long _rio_e21 = _rio_peer.ivec.get(_rio_vidx1);
        _rio_ret = (_rio_e11 == _rio_e21)? 0 :((_rio_e11<_rio_e21)?-1:1);
        if (_rio_ret != 0) { return _rio_ret; }
      }
      _rio_ret = (_rio_len11 - _rio_len21);
    }
    if (_rio_ret != 0) return _rio_ret;
    {
      int _rio_len11 = svec.size();
      int _rio_len21 = _rio_peer.svec.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len11 && _rio_vidx1<_rio_len21; _rio_vidx1++) {
        java.util.ArrayList<org.apache.hadoop.record.RecRecord0> _rio_e11 = svec.get(_rio_vidx1);
        java.util.ArrayList<org.apache.hadoop.record.RecRecord0> _rio_e21 = _rio_peer.svec.get(_rio_vidx1);
        {
          int _rio_len12 = _rio_e11.size();
          int _rio_len22 = _rio_e21.size();
          for(int _rio_vidx2 = 0; _rio_vidx2<_rio_len12 && _rio_vidx2<_rio_len22; _rio_vidx2++) {
            org.apache.hadoop.record.RecRecord0 _rio_e12 = _rio_e11.get(_rio_vidx2);
            org.apache.hadoop.record.RecRecord0 _rio_e22 = _rio_e21.get(_rio_vidx2);
            _rio_ret = _rio_e12.compareTo(_rio_e22);
            if (_rio_ret != 0) { return _rio_ret; }
          }
          _rio_ret = (_rio_len12 - _rio_len22);
        }
        if (_rio_ret != 0) { return _rio_ret; }
      }
      _rio_ret = (_rio_len11 - _rio_len21);
    }
    if (_rio_ret != 0) return _rio_ret;
    _rio_ret = inner.compareTo(_rio_peer.inner);
    if (_rio_ret != 0) return _rio_ret;
    {
      int _rio_len11 = strvec.size();
      int _rio_len21 = _rio_peer.strvec.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len11 && _rio_vidx1<_rio_len21; _rio_vidx1++) {
        java.util.ArrayList<java.util.ArrayList<String>> _rio_e11 = strvec.get(_rio_vidx1);
        java.util.ArrayList<java.util.ArrayList<String>> _rio_e21 = _rio_peer.strvec.get(_rio_vidx1);
        {
          int _rio_len12 = _rio_e11.size();
          int _rio_len22 = _rio_e21.size();
          for(int _rio_vidx2 = 0; _rio_vidx2<_rio_len12 && _rio_vidx2<_rio_len22; _rio_vidx2++) {
            java.util.ArrayList<String> _rio_e12 = _rio_e11.get(_rio_vidx2);
            java.util.ArrayList<String> _rio_e22 = _rio_e21.get(_rio_vidx2);
            {
              int _rio_len13 = _rio_e12.size();
              int _rio_len23 = _rio_e22.size();
              for(int _rio_vidx3 = 0; _rio_vidx3<_rio_len13 && _rio_vidx3<_rio_len23; _rio_vidx3++) {
                String _rio_e13 = _rio_e12.get(_rio_vidx3);
                String _rio_e23 = _rio_e22.get(_rio_vidx3);
                _rio_ret = _rio_e13.compareTo(_rio_e23);
                if (_rio_ret != 0) { return _rio_ret; }
              }
              _rio_ret = (_rio_len13 - _rio_len23);
            }
            if (_rio_ret != 0) { return _rio_ret; }
          }
          _rio_ret = (_rio_len12 - _rio_len22);
        }
        if (_rio_ret != 0) { return _rio_ret; }
      }
      _rio_ret = (_rio_len11 - _rio_len21);
    }
    if (_rio_ret != 0) return _rio_ret;
    _rio_ret = (i1 == _rio_peer.i1)? 0 :((i1<_rio_peer.i1)?-1:1);
    if (_rio_ret != 0) return _rio_ret;
    {
      java.util.Set<Byte> _rio_set10 = map1.keySet();
      java.util.Set<Byte> _rio_set20 = _rio_peer.map1.keySet();
      java.util.Iterator<Byte> _rio_miter10 = _rio_set10.iterator();
      java.util.Iterator<Byte> _rio_miter20 = _rio_set20.iterator();
      for(; _rio_miter10.hasNext() && _rio_miter20.hasNext();) {
        byte _rio_k10 = _rio_miter10.next();
        byte _rio_k20 = _rio_miter20.next();
        _rio_ret = (_rio_k10 == _rio_k20)? 0 :((_rio_k10<_rio_k20)?-1:1);
        if (_rio_ret != 0) { return _rio_ret; }
      }
      _rio_ret = (_rio_set10.size() - _rio_set20.size());
    }
    if (_rio_ret != 0) return _rio_ret;
    {
      int _rio_len11 = mvec1.size();
      int _rio_len21 = _rio_peer.mvec1.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len11 && _rio_vidx1<_rio_len21; _rio_vidx1++) {
        java.util.TreeMap<Integer,Long> _rio_e11 = mvec1.get(_rio_vidx1);
        java.util.TreeMap<Integer,Long> _rio_e21 = _rio_peer.mvec1.get(_rio_vidx1);
        {
          java.util.Set<Integer> _rio_set10 = _rio_e11.keySet();
          java.util.Set<Integer> _rio_set20 = _rio_e21.keySet();
          java.util.Iterator<Integer> _rio_miter10 = _rio_set10.iterator();
          java.util.Iterator<Integer> _rio_miter20 = _rio_set20.iterator();
          for(; _rio_miter10.hasNext() && _rio_miter20.hasNext();) {
            int _rio_k10 = _rio_miter10.next();
            int _rio_k20 = _rio_miter20.next();
            _rio_ret = (_rio_k10 == _rio_k20)? 0 :((_rio_k10<_rio_k20)?-1:1);
            if (_rio_ret != 0) { return _rio_ret; }
          }
          _rio_ret = (_rio_set10.size() - _rio_set20.size());
        }
        if (_rio_ret != 0) { return _rio_ret; }
      }
      _rio_ret = (_rio_len11 - _rio_len21);
    }
    if (_rio_ret != 0) return _rio_ret;
    {
      int _rio_len11 = mvec2.size();
      int _rio_len21 = _rio_peer.mvec2.size();
      for(int _rio_vidx1 = 0; _rio_vidx1<_rio_len11 && _rio_vidx1<_rio_len21; _rio_vidx1++) {
        java.util.TreeMap<Integer,Long> _rio_e11 = mvec2.get(_rio_vidx1);
        java.util.TreeMap<Integer,Long> _rio_e21 = _rio_peer.mvec2.get(_rio_vidx1);
        {
          java.util.Set<Integer> _rio_set10 = _rio_e11.keySet();
          java.util.Set<Integer> _rio_set20 = _rio_e21.keySet();
          java.util.Iterator<Integer> _rio_miter10 = _rio_set10.iterator();
          java.util.Iterator<Integer> _rio_miter20 = _rio_set20.iterator();
          for(; _rio_miter10.hasNext() && _rio_miter20.hasNext();) {
            int _rio_k10 = _rio_miter10.next();
            int _rio_k20 = _rio_miter20.next();
            _rio_ret = (_rio_k10 == _rio_k20)? 0 :((_rio_k10<_rio_k20)?-1:1);
            if (_rio_ret != 0) { return _rio_ret; }
          }
          _rio_ret = (_rio_set10.size() - _rio_set20.size());
        }
        if (_rio_ret != 0) { return _rio_ret; }
      }
      _rio_ret = (_rio_len11 - _rio_len21);
    }
    if (_rio_ret != 0) return _rio_ret;
    return _rio_ret;
  }
  public boolean equals(final Object _rio_peer_) {
    if (!(_rio_peer_ instanceof RecRecordOld)) {
      return false;
    }
    if (_rio_peer_ == this) {
      return true;
    }
    RecRecordOld _rio_peer = (RecRecordOld) _rio_peer_;
    boolean _rio_ret = false;
    _rio_ret = name.equals(_rio_peer.name);
    if (!_rio_ret) return _rio_ret;
    _rio_ret = ivec.equals(_rio_peer.ivec);
    if (!_rio_ret) return _rio_ret;
    _rio_ret = svec.equals(_rio_peer.svec);
    if (!_rio_ret) return _rio_ret;
    _rio_ret = inner.equals(_rio_peer.inner);
    if (!_rio_ret) return _rio_ret;
    _rio_ret = strvec.equals(_rio_peer.strvec);
    if (!_rio_ret) return _rio_ret;
    _rio_ret = (i1==_rio_peer.i1);
    if (!_rio_ret) return _rio_ret;
    _rio_ret = map1.equals(_rio_peer.map1);
    if (!_rio_ret) return _rio_ret;
    _rio_ret = mvec1.equals(_rio_peer.mvec1);
    if (!_rio_ret) return _rio_ret;
    _rio_ret = mvec2.equals(_rio_peer.mvec2);
    if (!_rio_ret) return _rio_ret;
    return _rio_ret;
  }
  public Object clone() throws CloneNotSupportedException {
    RecRecordOld _rio_other = new RecRecordOld();
    _rio_other.name = this.name;
    _rio_other.ivec = (java.util.ArrayList<Long>) this.ivec.clone();
    _rio_other.svec = (java.util.ArrayList<java.util.ArrayList<org.apache.hadoop.record.RecRecord0>>) this.svec.clone();
    _rio_other.inner = (org.apache.hadoop.record.RecRecord0) this.inner.clone();
    _rio_other.strvec = (java.util.ArrayList<java.util.ArrayList<java.util.ArrayList<String>>>) this.strvec.clone();
    _rio_other.i1 = this.i1;
    _rio_other.map1 = (java.util.TreeMap<Byte,String>) this.map1.clone();
    _rio_other.mvec1 = (java.util.ArrayList<java.util.TreeMap<Integer,Long>>) this.mvec1.clone();
    _rio_other.mvec2 = (java.util.ArrayList<java.util.TreeMap<Integer,Long>>) this.mvec2.clone();
    return _rio_other;
  }
  public int hashCode() {
    int _rio_result = 17;
    int _rio_ret;
    _rio_ret = name.hashCode();
    _rio_result = 37*_rio_result + _rio_ret;
    _rio_ret = ivec.hashCode();
    _rio_result = 37*_rio_result + _rio_ret;
    _rio_ret = svec.hashCode();
    _rio_result = 37*_rio_result + _rio_ret;
    _rio_ret = inner.hashCode();
    _rio_result = 37*_rio_result + _rio_ret;
    _rio_ret = strvec.hashCode();
    _rio_result = 37*_rio_result + _rio_ret;
    _rio_ret = Float.floatToIntBits(i1);
    _rio_result = 37*_rio_result + _rio_ret;
    _rio_ret = map1.hashCode();
    _rio_result = 37*_rio_result + _rio_ret;
    _rio_ret = mvec1.hashCode();
    _rio_result = 37*_rio_result + _rio_ret;
    _rio_ret = mvec2.hashCode();
    _rio_result = 37*_rio_result + _rio_ret;
    return _rio_result;
  }
  public static String signature() {
    return "LRecRecordOld(s[l][[LRecRecord0(s)]]LRecRecord0(s)[[[s]]]f{bs}[{il}][{il}])";
  }
  public static class Comparator extends org.apache.hadoop.record.RecordComparator {
    public Comparator() {
      super(RecRecordOld.class);
    }
    static public int slurpRaw(byte[] b, int s, int l) {
      try {
        int os = s;
        {
          int i = org.apache.hadoop.record.Utils.readVInt(b, s);
          int z = org.apache.hadoop.record.Utils.getVIntSize(i);
          s+=(z+i); l-= (z+i);
        }
        {
          int vi1 = org.apache.hadoop.record.Utils.readVInt(b, s);
          int vz1 = org.apache.hadoop.record.Utils.getVIntSize(vi1);
          s+=vz1; l-=vz1;
          for (int vidx1 = 0; vidx1 < vi1; vidx1++){
            long i = org.apache.hadoop.record.Utils.readVLong(b, s);
            int z = org.apache.hadoop.record.Utils.getVIntSize(i);
            s+=z; l-=z;
          }
        }
        {
          int vi1 = org.apache.hadoop.record.Utils.readVInt(b, s);
          int vz1 = org.apache.hadoop.record.Utils.getVIntSize(vi1);
          s+=vz1; l-=vz1;
          for (int vidx1 = 0; vidx1 < vi1; vidx1++){
            int vi2 = org.apache.hadoop.record.Utils.readVInt(b, s);
            int vz2 = org.apache.hadoop.record.Utils.getVIntSize(vi2);
            s+=vz2; l-=vz2;
            for (int vidx2 = 0; vidx2 < vi2; vidx2++){
              int r = org.apache.hadoop.record.RecRecord0.Comparator.slurpRaw(b,s,l);
              s+=r; l-=r;
            }
          }
        }
        {
          int r = org.apache.hadoop.record.RecRecord0.Comparator.slurpRaw(b,s,l);
          s+=r; l-=r;
        }
        {
          int vi1 = org.apache.hadoop.record.Utils.readVInt(b, s);
          int vz1 = org.apache.hadoop.record.Utils.getVIntSize(vi1);
          s+=vz1; l-=vz1;
          for (int vidx1 = 0; vidx1 < vi1; vidx1++){
            int vi2 = org.apache.hadoop.record.Utils.readVInt(b, s);
            int vz2 = org.apache.hadoop.record.Utils.getVIntSize(vi2);
            s+=vz2; l-=vz2;
            for (int vidx2 = 0; vidx2 < vi2; vidx2++){
              int vi3 = org.apache.hadoop.record.Utils.readVInt(b, s);
              int vz3 = org.apache.hadoop.record.Utils.getVIntSize(vi3);
              s+=vz3; l-=vz3;
              for (int vidx3 = 0; vidx3 < vi3; vidx3++){
                int i = org.apache.hadoop.record.Utils.readVInt(b, s);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s+=(z+i); l-= (z+i);
              }
            }
          }
        }
        {
          if (l<4) {
            throw new java.io.IOException("Float is exactly 4 bytes. Provided buffer is smaller.");
          }
          s+=4; l-=4;
        }
        {
          int mi1 = org.apache.hadoop.record.Utils.readVInt(b, s);
          int mz1 = org.apache.hadoop.record.Utils.getVIntSize(mi1);
          s+=mz1; l-=mz1;
          for (int midx1 = 0; midx1 < mi1; midx1++) {{
              if (l<1) {
                throw new java.io.IOException("Byte is exactly 1 byte. Provided buffer is smaller.");
              }
              s++; l--;
            }
            {
              int i = org.apache.hadoop.record.Utils.readVInt(b, s);
              int z = org.apache.hadoop.record.Utils.getVIntSize(i);
              s+=(z+i); l-= (z+i);
            }
          }
        }
        {
          int vi1 = org.apache.hadoop.record.Utils.readVInt(b, s);
          int vz1 = org.apache.hadoop.record.Utils.getVIntSize(vi1);
          s+=vz1; l-=vz1;
          for (int vidx1 = 0; vidx1 < vi1; vidx1++){
            int mi1 = org.apache.hadoop.record.Utils.readVInt(b, s);
            int mz1 = org.apache.hadoop.record.Utils.getVIntSize(mi1);
            s+=mz1; l-=mz1;
            for (int midx1 = 0; midx1 < mi1; midx1++) {{
                int i = org.apache.hadoop.record.Utils.readVInt(b, s);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s+=z; l-=z;
              }
              {
                long i = org.apache.hadoop.record.Utils.readVLong(b, s);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s+=z; l-=z;
              }
            }
          }
        }
        {
          int vi1 = org.apache.hadoop.record.Utils.readVInt(b, s);
          int vz1 = org.apache.hadoop.record.Utils.getVIntSize(vi1);
          s+=vz1; l-=vz1;
          for (int vidx1 = 0; vidx1 < vi1; vidx1++){
            int mi1 = org.apache.hadoop.record.Utils.readVInt(b, s);
            int mz1 = org.apache.hadoop.record.Utils.getVIntSize(mi1);
            s+=mz1; l-=mz1;
            for (int midx1 = 0; midx1 < mi1; midx1++) {{
                int i = org.apache.hadoop.record.Utils.readVInt(b, s);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s+=z; l-=z;
              }
              {
                long i = org.apache.hadoop.record.Utils.readVLong(b, s);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s+=z; l-=z;
              }
            }
          }
        }
        return (os - s);
      } catch(java.io.IOException e) {
        throw new RuntimeException(e);
      }
    }
    static public int compareRaw(byte[] b1, int s1, int l1,
                                   byte[] b2, int s2, int l2) {
      try {
        int os1 = s1;
        {
          int i1 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
          int i2 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
          int z1 = org.apache.hadoop.record.Utils.getVIntSize(i1);
          int z2 = org.apache.hadoop.record.Utils.getVIntSize(i2);
          s1+=z1; s2+=z2; l1-=z1; l2-=z2;
          int r1 = org.apache.hadoop.record.Utils.compareBytes(b1,s1,i1,b2,s2,i2);
          if (r1 != 0) { return (r1<0)?-1:0; }
          s1+=i1; s2+=i2; l1-=i1; l1-=i2;
        }
        {
          int vi11 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
          int vi21 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
          int vz11 = org.apache.hadoop.record.Utils.getVIntSize(vi11);
          int vz21 = org.apache.hadoop.record.Utils.getVIntSize(vi21);
          s1+=vz11; s2+=vz21; l1-=vz11; l2-=vz21;
          for (int vidx1 = 0; vidx1 < vi11 && vidx1 < vi21; vidx1++){
            long i1 = org.apache.hadoop.record.Utils.readVLong(b1, s1);
            long i2 = org.apache.hadoop.record.Utils.readVLong(b2, s2);
            if (i1 != i2) {
              return ((i1-i2) < 0) ? -1 : 0;
            }
            int z1 = org.apache.hadoop.record.Utils.getVIntSize(i1);
            int z2 = org.apache.hadoop.record.Utils.getVIntSize(i2);
            s1+=z1; s2+=z2; l1-=z1; l2-=z2;
          }
          if (vi11 != vi21) { return (vi11<vi21)?-1:0; }
        }
        {
          int vi11 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
          int vi21 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
          int vz11 = org.apache.hadoop.record.Utils.getVIntSize(vi11);
          int vz21 = org.apache.hadoop.record.Utils.getVIntSize(vi21);
          s1+=vz11; s2+=vz21; l1-=vz11; l2-=vz21;
          for (int vidx1 = 0; vidx1 < vi11 && vidx1 < vi21; vidx1++){
            int vi12 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
            int vi22 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
            int vz12 = org.apache.hadoop.record.Utils.getVIntSize(vi12);
            int vz22 = org.apache.hadoop.record.Utils.getVIntSize(vi22);
            s1+=vz12; s2+=vz22; l1-=vz12; l2-=vz22;
            for (int vidx2 = 0; vidx2 < vi12 && vidx2 < vi22; vidx2++){
              int r1 = org.apache.hadoop.record.RecRecord0.Comparator.compareRaw(b1,s1,l1,b2,s2,l2);
              if (r1 <= 0) { return r1; }
              s1+=r1; s2+=r1; l1-=r1; l2-=r1;
            }
            if (vi12 != vi22) { return (vi12<vi22)?-1:0; }
          }
          if (vi11 != vi21) { return (vi11<vi21)?-1:0; }
        }
        {
          int r1 = org.apache.hadoop.record.RecRecord0.Comparator.compareRaw(b1,s1,l1,b2,s2,l2);
          if (r1 <= 0) { return r1; }
          s1+=r1; s2+=r1; l1-=r1; l2-=r1;
        }
        {
          int vi11 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
          int vi21 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
          int vz11 = org.apache.hadoop.record.Utils.getVIntSize(vi11);
          int vz21 = org.apache.hadoop.record.Utils.getVIntSize(vi21);
          s1+=vz11; s2+=vz21; l1-=vz11; l2-=vz21;
          for (int vidx1 = 0; vidx1 < vi11 && vidx1 < vi21; vidx1++){
            int vi12 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
            int vi22 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
            int vz12 = org.apache.hadoop.record.Utils.getVIntSize(vi12);
            int vz22 = org.apache.hadoop.record.Utils.getVIntSize(vi22);
            s1+=vz12; s2+=vz22; l1-=vz12; l2-=vz22;
            for (int vidx2 = 0; vidx2 < vi12 && vidx2 < vi22; vidx2++){
              int vi13 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
              int vi23 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
              int vz13 = org.apache.hadoop.record.Utils.getVIntSize(vi13);
              int vz23 = org.apache.hadoop.record.Utils.getVIntSize(vi23);
              s1+=vz13; s2+=vz23; l1-=vz13; l2-=vz23;
              for (int vidx3 = 0; vidx3 < vi13 && vidx3 < vi23; vidx3++){
                int i1 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
                int i2 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
                int z1 = org.apache.hadoop.record.Utils.getVIntSize(i1);
                int z2 = org.apache.hadoop.record.Utils.getVIntSize(i2);
                s1+=z1; s2+=z2; l1-=z1; l2-=z2;
                int r1 = org.apache.hadoop.record.Utils.compareBytes(b1,s1,i1,b2,s2,i2);
                if (r1 != 0) { return (r1<0)?-1:0; }
                s1+=i1; s2+=i2; l1-=i1; l1-=i2;
              }
              if (vi13 != vi23) { return (vi13<vi23)?-1:0; }
            }
            if (vi12 != vi22) { return (vi12<vi22)?-1:0; }
          }
          if (vi11 != vi21) { return (vi11<vi21)?-1:0; }
        }
        {
          if (l1<4 || l2<4) {
            throw new java.io.IOException("Float is exactly 4 bytes. Provided buffer is smaller.");
          }
          float f1 = org.apache.hadoop.record.Utils.readFloat(b1, s1);
          float f2 = org.apache.hadoop.record.Utils.readFloat(b2, s2);
          if (f1 != f2) {
            return ((f1-f2) < 0) ? -1 : 0;
          }
          s1+=4; s2+=4; l1-=4; l2-=4;
        }
        {
          int mi11 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
          int mi21 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
          int mz11 = org.apache.hadoop.record.Utils.getVIntSize(mi11);
          int mz21 = org.apache.hadoop.record.Utils.getVIntSize(mi21);
          s1+=mz11; s2+=mz21; l1-=mz11; l2-=mz21;
          for (int midx1 = 0; midx1 < mi11 && midx1 < mi21; midx1++) {{
              if (l1<1 || l2<1) {
                throw new java.io.IOException("Byte is exactly 1 byte. Provided buffer is smaller.");
              }
              if (b1[s1] != b2[s2]) {
                return (b1[s1]<b2[s2])?-1:0;
              }
              s1++; s2++; l1--; l2--;
            }
            {
              int i = org.apache.hadoop.record.Utils.readVInt(b1, s1);
              int z = org.apache.hadoop.record.Utils.getVIntSize(i);
              s1+=(z+i); l1-= (z+i);
            }
            {
              int i = org.apache.hadoop.record.Utils.readVInt(b2, s2);
              int z = org.apache.hadoop.record.Utils.getVIntSize(i);
              s2+=(z+i); l2-= (z+i);
            }
          }
          if (mi11 != mi21) { return (mi11<mi21)?-1:0; }
        }
        {
          int vi11 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
          int vi21 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
          int vz11 = org.apache.hadoop.record.Utils.getVIntSize(vi11);
          int vz21 = org.apache.hadoop.record.Utils.getVIntSize(vi21);
          s1+=vz11; s2+=vz21; l1-=vz11; l2-=vz21;
          for (int vidx1 = 0; vidx1 < vi11 && vidx1 < vi21; vidx1++){
            int mi11 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
            int mi21 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
            int mz11 = org.apache.hadoop.record.Utils.getVIntSize(mi11);
            int mz21 = org.apache.hadoop.record.Utils.getVIntSize(mi21);
            s1+=mz11; s2+=mz21; l1-=mz11; l2-=mz21;
            for (int midx1 = 0; midx1 < mi11 && midx1 < mi21; midx1++) {{
                int i1 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
                int i2 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
                if (i1 != i2) {
                  return ((i1-i2) < 0) ? -1 : 0;
                }
                int z1 = org.apache.hadoop.record.Utils.getVIntSize(i1);
                int z2 = org.apache.hadoop.record.Utils.getVIntSize(i2);
                s1+=z1; s2+=z2; l1-=z1; l2-=z2;
              }
              {
                long i = org.apache.hadoop.record.Utils.readVLong(b1, s1);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s1+=z; l1-=z;
              }
              {
                long i = org.apache.hadoop.record.Utils.readVLong(b2, s2);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s2+=z; l2-=z;
              }
            }
            if (mi11 != mi21) { return (mi11<mi21)?-1:0; }
          }
          if (vi11 != vi21) { return (vi11<vi21)?-1:0; }
        }
        {
          int vi11 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
          int vi21 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
          int vz11 = org.apache.hadoop.record.Utils.getVIntSize(vi11);
          int vz21 = org.apache.hadoop.record.Utils.getVIntSize(vi21);
          s1+=vz11; s2+=vz21; l1-=vz11; l2-=vz21;
          for (int vidx1 = 0; vidx1 < vi11 && vidx1 < vi21; vidx1++){
            int mi11 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
            int mi21 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
            int mz11 = org.apache.hadoop.record.Utils.getVIntSize(mi11);
            int mz21 = org.apache.hadoop.record.Utils.getVIntSize(mi21);
            s1+=mz11; s2+=mz21; l1-=mz11; l2-=mz21;
            for (int midx1 = 0; midx1 < mi11 && midx1 < mi21; midx1++) {{
                int i1 = org.apache.hadoop.record.Utils.readVInt(b1, s1);
                int i2 = org.apache.hadoop.record.Utils.readVInt(b2, s2);
                if (i1 != i2) {
                  return ((i1-i2) < 0) ? -1 : 0;
                }
                int z1 = org.apache.hadoop.record.Utils.getVIntSize(i1);
                int z2 = org.apache.hadoop.record.Utils.getVIntSize(i2);
                s1+=z1; s2+=z2; l1-=z1; l2-=z2;
              }
              {
                long i = org.apache.hadoop.record.Utils.readVLong(b1, s1);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s1+=z; l1-=z;
              }
              {
                long i = org.apache.hadoop.record.Utils.readVLong(b2, s2);
                int z = org.apache.hadoop.record.Utils.getVIntSize(i);
                s2+=z; l2-=z;
              }
            }
            if (mi11 != mi21) { return (mi11<mi21)?-1:0; }
          }
          if (vi11 != vi21) { return (vi11<vi21)?-1:0; }
        }
        return (os1 - s1);
      } catch(java.io.IOException e) {
        throw new RuntimeException(e);
      }
    }
    public int compare(byte[] b1, int s1, int l1,
                         byte[] b2, int s2, int l2) {
      int ret = compareRaw(b1,s1,l1,b2,s2,l2);
      return (ret == -1)? -1 : ((ret==0)? 1 : 0);}
  }
  
  static {
    org.apache.hadoop.record.RecordComparator.define(RecRecordOld.class, new Comparator());
  }
}
